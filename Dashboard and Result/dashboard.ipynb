{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTxBjDUhThj_",
        "outputId": "e64ecb10-9a0a-4bee-92f4-4634e999d5a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "added 22 packages in 3s\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "!pip install streamlit pandas seaborn matplotlib -q\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set page configuration\n",
        "st.set_page_config(page_title=\"Inference Optimization Dashboard\", page_icon=\"📊\", layout=\"wide\")\n",
        "\n",
        "# Title and description\n",
        "st.title(\"Inference Optimization Dashboard\")\n",
        "st.write(\"This dashboard visualizes performance metrics (latency, throughput, memory usage) for various models, comparing CPU/GPU and quantization configurations. Use the sidebar to filter models or quantization types.\")\n",
        "\n",
        "# Data preparation\n",
        "data = {\n",
        "    \"Model\": [\"Whisper-base\", \"Whisper-base\", \"Whisper-medium\", \"Whisper-medium\", \"Whisper-medium\", \"Whisper-medium\", \"Whisper-medium\", \"Gemma 2-2B\", \"Mistral 7B\", \"Paligemma\", \"MiniCPM-V 2.6\", \"MiniCPM-V 2.6\"],\n",
        "    \"Prompt\": [\"Transcription\", \"Transcription\", \"Transcription\", \"Transcription\", \"Transcription\", \"Transcription\", \"Transcription\", \"Transcription\", \"What is the meaning of life?\", \"Describe the image.\", \"What is the meaning of life?\", \"What is the meaning of life?\"],\n",
        "    \"Batch Size\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "    \"Quantized\": [\"No\", \"No\", \"No (FP32)\", \"No (FP32)\", \"Yes (INT8)\", \"Yes (4-bit)\", \"Yes (FP16)\", \"Yes (4-bit)\", \"Yes (4-bit)\", \"Yes (4-bit)\", \"No (FP16)\", \"Yes (4-bit)\"],\n",
        "    \"Tensor Parallel\": [\"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"],\n",
        "    \"Latency (s)\": [0.2498, 5.9084, 1.3950, 45.3059, 29.2023, 2.7448, 1.0, 2.7448, 217.40, 2.10, 1.82, 2.05],\n",
        "    \"Throughput (tokens/s)\": [None, None, None, None, None, 24.44, None, 24.44, 0.23, 0.95, 16.48, 14.63],\n",
        "    \"Notes\": [\n",
        "        \"GPU (T4), peak memory 330.93 MB\",\n",
        "        \"CPU, memory 0.46 MB\",\n",
        "        \"GPU (T4), peak memory 4103.10 MB\",\n",
        "        \"CPU, memory 5.41 MB\",\n",
        "        \"CPU, memory 8463.5625 MB\",\n",
        "        \"GPU (T4), peak memory 4060.91 MB, bitsandbytes\",\n",
        "        \"GPU (T4), peak memory 2.1 GB\",\n",
        "        \"GPU (T4), peak memory 4060.91 MB, bitsandbytes\",\n",
        "        \"GPU (T4), memory 3945.98 MB\",\n",
        "        \"GPU (T4), memory 3380.03 MB\",\n",
        "        \"GPU (T4), memory 8972.4 MB\",\n",
        "        \"GPU (T4), memory 3245.7 MB, 63.8% memory reduction\"\n",
        "    ],\n",
        "    \"Hardware\": [\"GPU\", \"CPU\", \"GPU\", \"CPU\", \"CPU\", \"GPU\", \"GPU\", \"GPU\", \"GPU\", \"GPU\", \"GPU\", \"GPU\"],\n",
        "    \"Memory (MB)\": [330.93, 0.46, 4103.10, 5.41, 8463.5625, 4060.91, 2100, 4060.91, 3945.98, 3380.03, 8972.4, 3245.7]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Sidebar filters\n",
        "st.sidebar.header(\"Filter Options\")\n",
        "selected_models = st.sidebar.multiselect(\"Select Models\", options=df[\"Model\"].unique(), default=df[\"Model\"].unique())\n",
        "quantization_filter = st.sidebar.multiselect(\"Select Quantization Types\", options=df[\"Quantized\"].unique(), default=df[\"Quantized\"].unique())\n",
        "\n",
        "# Filter data\n",
        "filtered_df = df[df[\"Model\"].isin(selected_models) & df[\"Quantized\"].isin(quantization_filter)]\n",
        "\n",
        "# Latency comparison\n",
        "st.subheader(\"Latency Comparison (Lower is Better)\")\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.barplot(data=filtered_df, x=\"Latency (s)\", y=\"Model\", hue=\"Notes\", ax=ax)\n",
        "ax.set_xlabel(\"Latency (seconds)\")\n",
        "ax.set_ylabel(\"Model\")\n",
        "ax.set_title(\"Latency by Model and Configuration\")\n",
        "plt.tight_layout()\n",
        "st.pyplot(fig)\n",
        "\n",
        "# Throughput comparison (where available)\n",
        "st.subheader(\"Throughput Comparison (Higher is Better)\")\n",
        "throughput_df = filtered_df[filtered_df[\"Throughput (tokens/s)\"].notnull()]\n",
        "if not throughput_df.empty:\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.barplot(data=throughput_df, x=\"Throughput (tokens/s)\", y=\"Model\", hue=\"Notes\", ax=ax)\n",
        "    ax.set_xlabel(\"Throughput (tokens/second)\")\n",
        "    ax.set_ylabel(\"Model\")\n",
        "    ax.set_title(\"Throughput by Model and Configuration\")\n",
        "    plt.tight_layout()\n",
        "    st.pyplot(fig)\n",
        "else:\n",
        "    st.write(\"No throughput data available for selected filters.\")\n",
        "\n",
        "# Latency vs Memory Scatter Plot\n",
        "st.subheader(\"Latency vs Memory Usage\")\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.scatterplot(data=filtered_df, x=\"Latency (s)\", y=\"Memory (MB)\", hue=\"Model\", style=\"Hardware\", size=\"Memory (MB)\", ax=ax)\n",
        "ax.set_xlabel(\"Latency (seconds)\")\n",
        "ax.set_ylabel(\"Memory Usage (MB)\")\n",
        "ax.set_title(\"Latency vs Memory Usage by Model and Hardware\")\n",
        "plt.tight_layout()\n",
        "st.pyplot(fig)\n",
        "\n",
        "# Raw data display\n",
        "st.subheader(\"Raw Data\")\n",
        "st.write(filtered_df)\n",
        "\n",
        "# Acknowledgements\n",
        "st.subheader(\"Acknowledgements\")\n",
        "st.write(\"Built with [Streamlit](https://streamlit.io) and visualized using [Seaborn](https://seaborn.pydata.org). Data sourced from model inference experiments.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNuJvE50Tzd2",
        "outputId": "72dd4697-7f91-4d6a-904c-02b2bae81537"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pandas seaborn matplotlib pyngrok -q"
      ],
      "metadata": {
        "id": "H4MDH-BFUwqN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "# Set your Ngrok authtoken\n",
        "!ngrok authtoken 2wqNxcPxPAmko5XPrXkDZUPWYPb_4ydJ85uD4WpkCicbZ3dAw  # Replace with your actual token\n",
        "\n",
        "# Terminate any existing Ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start Streamlit in the background\n",
        "subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"])\n",
        "\n",
        "# Create a public URL with Ngrok\n",
        "public_url = ngrok.connect(8501, bind_tls=True)\n",
        "print(f\"Your Streamlit app is live at: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aUhWnCkU0os",
        "outputId": "6ad999a9-19e8-405a-8c79-0c61cd52fcff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Your Streamlit app is live at: NgrokTunnel: \"https://c346-34-16-234-84.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyngrok import ngrok\n",
        "# ngrok.kill()\n",
        "# print(\"All Ngrok tunnels have been terminated.\")"
      ],
      "metadata": {
        "id": "bW-Y6LVKbLMx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2wqNxcPxPAmko5XPrXkDZUPWYPb_4ydJ85uD4WpkCicbZ3dAw"
      ],
      "metadata": {
        "id": "ifUlylLfVmXS"
      }
    }
  ]
}