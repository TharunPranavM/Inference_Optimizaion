{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_9UI6haagmE",
        "outputId": "7dbce945-c19f-4381-874a-fb5175c3e888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install streamlit pandas seaborn matplotlib pyngrok -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "\n",
        "# Set page configuration\n",
        "st.set_page_config(page_title=\"Continuous Batching Comparison Dashboard\", page_icon=\"ðŸ“ˆ\", layout=\"wide\")\n",
        "\n",
        "# Title and description\n",
        "st.title(\"Continuous Batching Comparison Dashboard\")\n",
        "st.write(\"This dashboard visualizes latency and throughput across different batch sizes for distilgpt2 and Llama-3-8B-Instruct-GPTQ-4-Bit. Select a model to view line charts comparing batch sizes. Lower latency and higher throughput are better.\")\n",
        "\n",
        "# Load CSV data\n",
        "csv_data = \"\"\"Model,Batch Size,Quantized,Hardware,Latency (s),Throughput (tokens/s),Notes\n",
        "distilgpt2,1,No,GPU,0.36,140.71,\"VLLM v0 engine (fallback on T4), XFormers backend\"\n",
        "distilgpt2,5,No,GPU,0.64,350.42,\"VLLM v0 engine (T4 GPU), dynamic batching, 5 prompts processed in parallel\"\n",
        "distilgpt2,1,No,GPU,0.56,36.07,\"VLLM v0 engine, XFormers backend\"\n",
        "distilgpt2,2,No,GPU,0.44,44.54,\"VLLM v0 engine, optimal latency\"\n",
        "distilgpt2,4,No,GPU,0.49,92.36,\"VLLM v0 engine, incomplete outputs observed\"\n",
        "distilgpt2,8,No,GPU,0.53,174.88,\"VLLM v0 engine, only 5 prompts processed\"\n",
        "distilgpt2,16,No,GPU,0.70,208.97,\"VLLM v0 engine, only 5 prompts processed, max tokens=64\"\n",
        "Llama-3-8B-Instruct-GPTQ-4-Bit,1,Yes (4-bit),GPU,6.92,21.67,\"VLLM engine, T4 GPU\"\n",
        "Llama-3-8B-Instruct-GPTQ-4-Bit,2,Yes (4-bit),GPU,6.69,44.53,\"VLLM engine, T4 GPU\"\n",
        "Llama-3-8B-Instruct-GPTQ-4-Bit,4,Yes (4-bit),GPU,7.67,92.72,\"VLLM engine, T4 GPU\"\n",
        "Llama-3-8B-Instruct-GPTQ-4-Bit,8,Yes (4-bit),GPU,10.32,144.33,\"VLLM engine, T4 GPU\"\n",
        "Llama-3-8B-Instruct-GPTQ-4-Bit,16,Yes (4-bit),GPU,18.68,159.81,\"VLLM engine, T4 GPU, 7.4x throughput improvement\"\n",
        "\"\"\"\n",
        "df = pd.read_csv(io.StringIO(csv_data))\n",
        "\n",
        "# Sidebar model selection\n",
        "st.sidebar.header(\"Select Model\")\n",
        "model = st.sidebar.radio(\"Choose a model to visualize:\", options=df[\"Model\"].unique(), index=0)\n",
        "\n",
        "# Filter data by selected model\n",
        "filtered_df = df[df[\"Model\"] == model].sort_values(\"Batch Size\")\n",
        "\n",
        "# Latency line chart\n",
        "st.subheader(f\"Latency vs Batch Size for {model} (Lower is Better)\")\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.lineplot(data=filtered_df, x=\"Batch Size\", y=\"Latency (s)\", marker=\"o\", ax=ax)\n",
        "ax.set_xlabel(\"Batch Size\")\n",
        "ax.set_ylabel(\"Latency (seconds)\")\n",
        "ax.set_title(f\"Latency Across Batch Sizes for {model}\")\n",
        "for i, row in filtered_df.iterrows():\n",
        "    ax.annotate(f\"{row['Latency (s)']:.2f}\", (row[\"Batch Size\"], row[\"Latency (s)\"]), textcoords=\"offset points\", xytext=(0,10), ha=\"center\")\n",
        "ax.grid(True)\n",
        "plt.tight_layout()\n",
        "st.pyplot(fig)\n",
        "\n",
        "# Throughput line chart\n",
        "st.subheader(f\"Throughput vs Batch Size for {model} (Higher is Better)\")\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "sns.lineplot(data=filtered_df, x=\"Batch Size\", y=\"Throughput (tokens/s)\", marker=\"o\", ax=ax)\n",
        "ax.set_xlabel(\"Batch Size\")\n",
        "ax.set_ylabel(\"Throughput (tokens/second)\")\n",
        "ax.set_title(f\"Throughput Across Batch Sizes for {model}\")\n",
        "for i, row in filtered_df.iterrows():\n",
        "    ax.annotate(f\"{row['Throughput (tokens/s)']:.2f}\", (row[\"Batch Size\"], row[\"Throughput (tokens/s)\"]), textcoords=\"offset points\", xytext=(0,10), ha=\"center\")\n",
        "ax.grid(True)\n",
        "plt.tight_layout()\n",
        "st.pyplot(fig)\n",
        "\n",
        "# Raw data display\n",
        "st.subheader(f\"Raw Data for {model}\")\n",
        "st.write(filtered_df[[\"Batch Size\", \"Quantized\", \"Hardware\", \"Latency (s)\", \"Throughput (tokens/s)\", \"Notes\"]])\n",
        "\n",
        "# Acknowledgements\n",
        "st.subheader(\"Acknowledgements\")\n",
        "st.write(\"Built with [Streamlit](https://streamlit.io) and visualized using [Seaborn](https://seaborn.pydata.org). Data sourced from continuous batching experiments.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlLgA226axPs",
        "outputId": "146ee170-166b-4324-b26d-c577ac0242fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "# Set your Ngrok authtoken\n",
        "!ngrok authtoken 2wqNxcPxPAmko5XPrXkDZUPWYPb_4ydJ85uD4WpkCicbZ3dAw  # Replace with your actual token\n",
        "\n",
        "# Terminate any existing Ngrok tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start Streamlit in the background\n",
        "subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"])\n",
        "\n",
        "# Create a public URL with Ngrok\n",
        "public_url = ngrok.connect(8501, bind_tls=True)\n",
        "print(f\"Your Streamlit app is live at: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg4WHSNja1Oc",
        "outputId": "5a856866-2892-494b-cac5-d5e6d0fd531d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Your Streamlit app is live at: NgrokTunnel: \"https://4794-34-142-245-120.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()\n",
        "print(\"All Ngrok tunnels have been terminated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWmkuWjpbc4l",
        "outputId": "22674082-5d65-4651-c502-e2bf51653ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All Ngrok tunnels have been terminated.\n"
          ]
        }
      ]
    }
  ]
}