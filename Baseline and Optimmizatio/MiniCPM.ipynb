{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes==0.43.0 transformers accelerate pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwgGOSXm1_uN",
        "outputId": "7fad457a-231c-4194-d1b1-913ffb51391d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzcV0F5y0A5j"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import AutoModel, AutoProcessor, BitsAndBytesConfig\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    is_colab = True\n",
        "except ImportError:\n",
        "    is_colab = False\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# Verify GPU is available\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Model: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"⚠️ No GPU detected! Please enable GPU in Colab.\")\n",
        "    print(\"Go to: Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
        "    print(\"Then restart the runtime: Runtime > Restart runtime\")\n",
        "\n",
        "# Verify bitsandbytes installation\n",
        "try:\n",
        "    import bitsandbytes as bnb\n",
        "    print(f\"✅ bitsandbytes version: {bnb.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"❌ bitsandbytes not installed correctly\")\n",
        "    print(\"Try installing with: !pip install bitsandbytes==0.43.0\")\n",
        "\n",
        "# Retrieve Hugging Face token (optional)\n",
        "if is_colab:\n",
        "    try:\n",
        "        os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "        print(\"HF_TOKEN found and set\")\n",
        "    except:\n",
        "        print(\"HF_TOKEN not found, proceeding without it.\")\n",
        "\n",
        "# Function to measure memory usage\n",
        "def get_memory_usage():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.memory_allocated() / 1024**2  # MB\n",
        "    return 0\n",
        "\n",
        "# Function to clear memory\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Input prompt (text-only)\n",
        "prompt = \"What is the meaning of life?\"\n",
        "\n",
        "# Optional: Image input (uncomment to use)\n",
        "\"\"\"\n",
        "image_url = \"https://picsum.photos/512\"\n",
        "try:\n",
        "    response = requests.get(image_url, stream=True)\n",
        "    image = Image.open(response.raw).convert(\"RGB\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading image: {str(e)}\")\n",
        "    exit(1)\n",
        "prompt = \"Describe the image.\"\n",
        "\"\"\"\n",
        "\n",
        "# Create cache directory if it doesn't exist\n",
        "os.makedirs(\"/content/model_cache\", exist_ok=True)\n",
        "\n",
        "# --- FP16 Setup ---\n",
        "print(\"\\n=== MiniCPM-V 2.6 FP16 ===\")\n",
        "fp16_load_start = time.time()\n",
        "\n",
        "try:\n",
        "    fp16_model = AutoModel.from_pretrained(\n",
        "        \"openbmb/MiniCPM-V-2_6\",\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        cache_dir=\"/content/model_cache\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    fp16_processor = AutoProcessor.from_pretrained(\n",
        "        \"openbmb/MiniCPM-V-2_6\",\n",
        "        cache_dir=\"/content/model_cache\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    fp16_load_time = time.time() - fp16_load_start\n",
        "    print(f\"FP16 Loading Time: {fp16_load_time:.2f} seconds\")\n",
        "\n",
        "    # Inference\n",
        "    fp16_inf_start = time.time()\n",
        "    inputs = fp16_processor(text=prompt, images=None, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = fp16_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=30,\n",
        "            do_sample=False,\n",
        "            num_beams=1\n",
        "        )\n",
        "    fp16_output = fp16_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "    fp16_inf_latency = time.time() - fp16_inf_start\n",
        "    fp16_memory = get_memory_usage()\n",
        "    fp16_tokens = len(outputs[0]) - inputs[\"input_ids\"].shape[1]\n",
        "    fp16_throughput = fp16_tokens / fp16_inf_latency if fp16_inf_latency > 0 else 0.0\n",
        "\n",
        "    print(\"Output:\", fp16_output)\n",
        "    print(f\"Inference Latency: {fp16_inf_latency:.2f} seconds\")\n",
        "    print(f\"Throughput: {fp16_throughput:.2f} tokens/second\")\n",
        "    print(f\"Memory Usage: {fp16_memory:.2f} MB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in FP16 setup: {str(e)}\")\n",
        "    fp16_output = \"Failed\"\n",
        "    fp16_load_time = fp16_inf_latency = fp16_throughput = fp16_memory = 0.0\n",
        "\n",
        "# Clear GPU memory\n",
        "try:\n",
        "    del fp16_model\n",
        "    del fp16_processor\n",
        "except:\n",
        "    pass\n",
        "clear_memory()\n",
        "\n",
        "# --- Quantized Setup (4-bit, bitsandbytes) ---\n",
        "print(\"\\n=== MiniCPM-V 2.6 Quantized (4-bit, bitsandbytes) ===\")\n",
        "quant_load_start = time.time()\n",
        "\n",
        "try:\n",
        "    # Make sure bitsandbytes is imported\n",
        "    import bitsandbytes as bnb\n",
        "\n",
        "    # Define quantization configuration\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "\n",
        "    # Load quantized model\n",
        "    quant_model = AutoModel.from_pretrained(\n",
        "        \"openbmb/MiniCPM-V-2_6\",\n",
        "        quantization_config=quant_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        cache_dir=\"/content/model_cache\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Load processor\n",
        "    quant_processor = AutoProcessor.from_pretrained(\n",
        "        \"openbmb/MiniCPM-V-2_6\",\n",
        "        cache_dir=\"/content/model_cache\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    quant_load_time = time.time() - quant_load_start\n",
        "    print(f\"Quantized Loading Time: {quant_load_time:.2f} seconds\")\n",
        "\n",
        "    # Inference\n",
        "    quant_inf_start = time.time()\n",
        "    inputs = quant_processor(text=prompt, images=None, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = quant_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=30,\n",
        "            do_sample=False,\n",
        "            num_beams=1\n",
        "        )\n",
        "    quant_output = quant_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "    quant_inf_latency = time.time() - quant_inf_start\n",
        "    quant_memory = get_memory_usage()\n",
        "    quant_tokens = len(outputs[0]) - inputs[\"input_ids\"].shape[1]\n",
        "    quant_throughput = quant_tokens / quant_inf_latency if quant_inf_latency > 0 else 0.0\n",
        "\n",
        "    print(\"Output:\", quant_output)\n",
        "    print(f\"Inference Latency: {quant_inf_latency:.2f} seconds\")\n",
        "    print(f\"Throughput: {quant_throughput:.2f} tokens/second\")\n",
        "    print(f\"Memory Usage: {quant_memory:.2f} MB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in quantized setup: {str(e)}\")\n",
        "    quant_output = \"Failed\"\n",
        "    quant_load_time = quant_inf_latency = quant_throughput = quant_memory = 0.0\n",
        "\n",
        "# Clear GPU memory\n",
        "try:\n",
        "    del quant_model\n",
        "    del quant_processor\n",
        "except:\n",
        "    pass\n",
        "clear_memory()\n",
        "\n",
        "# Print comparison (if both versions ran successfully)\n",
        "if fp16_output != \"Failed\" and quant_output != \"Failed\":\n",
        "    print(\"\\n=== Performance Comparison ===\")\n",
        "    print(f\"                | FP16      | 4-bit\")\n",
        "    print(f\"----------------|-----------|-----------\")\n",
        "    print(f\"Loading Time    | {fp16_load_time:.2f}s     | {quant_load_time:.2f}s\")\n",
        "    print(f\"Inference Time  | {fp16_inf_latency:.2f}s     | {quant_inf_latency:.2f}s\")\n",
        "    print(f\"Throughput      | {fp16_throughput:.2f} t/s | {quant_throughput:.2f} t/s\")\n",
        "    print(f\"Memory Usage    | {fp16_memory:.2f} MB  | {quant_memory:.2f} MB\")\n",
        "\n",
        "    # Calculate memory savings\n",
        "    if fp16_memory > 0:\n",
        "        memory_savings = (1 - (quant_memory / fp16_memory)) * 100\n",
        "        print(f\"Memory Savings: {memory_savings:.1f}%\")\n",
        "\n",
        "    # Calculate speed impact\n",
        "    if fp16_throughput > 0:\n",
        "        speed_impact = ((quant_throughput / fp16_throughput) - 1) * 100\n",
        "        print(f\"Speed Impact: {speed_impact:.1f}% ({'faster' if speed_impact > 0 else 'slower'})\")"
      ]
    }
  ]
}